{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63227b0",
   "metadata": {},
   "source": [
    "# Notebook 2 — Preparação do Dataset para RAG\n",
    "\n",
    "Este notebook aplica as correções e padronizações necessárias para deixar o dataset pronto para uso em um pipeline de RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "Nesta etapa:\n",
    "- Corrigimos registros com inventário incorreto (ex.: PDF apontando para path inválido).\n",
    "- Padronizamos campos textuais e metadados.\n",
    "- Tratamos duplicados (mantendo os intencionais, com versionamento).\n",
    "- Geramos o arquivo final `rag_dataset.csv`.\n",
    "- Registramos todas as correções em logs para rastreabilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4726b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH existe? True\n",
      "CSV existe? True\n",
      "Pastas: ['fichas_tecnicas', 'imagens', 'inventario_curado.csv', 'inventario_dataset.csv', 'rag_dataset_chunks.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# notebook está em .../atividade6/notebooks\n",
    "BASE_PATH = Path(os.getcwd()).parent              # .../atividade6\n",
    "DATASET_PATH = BASE_PATH / \"dataset_restaurante\"  # .../atividade6/dataset_restaurante\n",
    "\n",
    "# Aqui é a \"raiz\" dos arquivos (PDFs e imagens)\n",
    "DATA_ROOT = DATASET_PATH\n",
    "\n",
    "CSV_PATH = DATASET_PATH / \"inventario_dataset.csv\"\n",
    "\n",
    "print(\"DATASET_PATH existe?\", DATASET_PATH.exists())\n",
    "print(\"CSV existe?\", CSV_PATH.exists())\n",
    "print(\"Pastas:\", [p.name for p in DATASET_PATH.iterdir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22ab9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas no inventário: 54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>titulo</th>\n",
       "      <th>origem</th>\n",
       "      <th>data</th>\n",
       "      <th>categoria</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "      <td>Favada</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-12-04</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_003</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_03_feijao_de_corda.pdf</td>\n",
       "      <td>Feijao-de-Corda</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDF_004</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_04_sarapatel.pdf</td>\n",
       "      <td>Sarapatel</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>15/03/2025</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDF_005</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_05_caldo_de_mocoto.pdf</td>\n",
       "      <td>Caldo de Mocoto</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>11-05-2025</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id tipo                                  path_arquivo  \\\n",
       "0     PDF_001  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf   \n",
       "1     PDF_002  pdf           fichas_tecnicas/ficha_02_favada.pdf   \n",
       "2     PDF_003  pdf  fichas_tecnicas/ficha_03_feijao_de_corda.pdf   \n",
       "3     PDF_004  pdf        fichas_tecnicas/ficha_04_sarapatel.pdf   \n",
       "4     PDF_005  pdf  fichas_tecnicas/ficha_05_caldo_de_mocoto.pdf   \n",
       "\n",
       "            titulo                 origem        data     categoria versao  \\\n",
       "0    Baiao-de-Dois  Ficha técnica oficial  2025-02-05  Tradicionais   v1.0   \n",
       "1           Favada  Ficha técnica oficial  2025-12-04  Tradicionais   v1.0   \n",
       "2  Feijao-de-Corda  Ficha técnica oficial  2025-05-31  Tradicionais   v1.0   \n",
       "3        Sarapatel  Ficha técnica oficial  15/03/2025  Tradicionais   v1.0   \n",
       "4  Caldo de Mocoto  Ficha técnica oficial  11-05-2025  Tradicionais   v1.0   \n",
       "\n",
       "  nivel_confianca  \n",
       "0            alto  \n",
       "1            alto  \n",
       "2            alto  \n",
       "3            alto  \n",
       "4            alto  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Linhas no inventário:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efc2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def existe(path_rel: str) -> bool:\n",
    "    return (DATA_ROOT / str(path_rel)).exists()\n",
    "\n",
    "def normalizar_texto(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    x = str(x).strip()\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c09ecd",
   "metadata": {},
   "source": [
    "## Etapa 1 — Correção dirigida do PDF_021\n",
    "\n",
    "O documento `PDF_021` estava com tipo \"pdf\", porém apontando para um caminho incorreto (imagem inexistente).\n",
    "Aqui corrigimos o `path_arquivo` para a ficha técnica correta do **Creme brûlée de doce de leite** e registramos a alteração em log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a8577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>campo</th>\n",
       "      <th>valor_antigo</th>\n",
       "      <th>valor_novo</th>\n",
       "      <th>arquivo_existe_depois</th>\n",
       "      <th>observacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_021</td>\n",
       "      <td>path_arquivo</td>\n",
       "      <td>imagens/arquivo_nao_existe.jpg</td>\n",
       "      <td>fichas_tecnicas/ficha_21_creme_brulee_de_doce_...</td>\n",
       "      <td>True</td>\n",
       "      <td>Correção do inventário: PDF apontava para path...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id         campo                    valor_antigo  \\\n",
       "0     PDF_021  path_arquivo  imagens/arquivo_nao_existe.jpg   \n",
       "\n",
       "                                          valor_novo  arquivo_existe_depois  \\\n",
       "0  fichas_tecnicas/ficha_21_creme_brulee_de_doce_...                   True   \n",
       "\n",
       "                                          observacao  \n",
       "0  Correção do inventário: PDF apontava para path...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_CORRETO_PDF21 = \"fichas_tecnicas/ficha_21_creme_brulee_de_doce_de_leite.pdf\"\n",
    "\n",
    "# antes\n",
    "path_antigo = df.loc[df[\"document_id\"] == \"PDF_021\", \"path_arquivo\"].iloc[0]\n",
    "\n",
    "# aplica correção\n",
    "df.loc[df[\"document_id\"] == \"PDF_021\", \"path_arquivo\"] = PATH_CORRETO_PDF21\n",
    "df.loc[df[\"document_id\"] == \"PDF_021\", \"tipo\"] = \"pdf\"\n",
    "\n",
    "log_fix_pdf21 = pd.DataFrame([{\n",
    "    \"document_id\": \"PDF_021\",\n",
    "    \"campo\": \"path_arquivo\",\n",
    "    \"valor_antigo\": path_antigo,\n",
    "    \"valor_novo\": PATH_CORRETO_PDF21,\n",
    "    \"arquivo_existe_depois\": existe(PATH_CORRETO_PDF21),\n",
    "    \"observacao\": \"Correção do inventário: PDF apontava para path incorreto\"\n",
    "}])\n",
    "\n",
    "log_fix_pdf21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab2d1f",
   "metadata": {},
   "source": [
    "ETAPA 2 — Padronização definitiva das nomenclaturas das imagens IMG_012 e IMG_022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1fac22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>arquivo_existe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>IMG_012</td>\n",
       "      <td>imagem</td>\n",
       "      <td>imagens/IMG_12.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IMG_022</td>\n",
       "      <td>imagem</td>\n",
       "      <td>imagens/img_22pratofinal.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id    tipo                  path_arquivo  arquivo_existe\n",
       "37     IMG_012  imagem            imagens/IMG_12.jpg           False\n",
       "47     IMG_022  imagem  imagens/img_22pratofinal.jpg           False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"document_id\"].isin([\"IMG_012\", \"IMG_022\"]),\n",
    "       [\"document_id\", \"tipo\", \"path_arquivo\"]].assign(\n",
    "    arquivo_existe=lambda x: x[\"path_arquivo\"].apply(existe)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa13533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGENS_DIR = DATASET_PATH / \"imagens\"\n",
    "\n",
    "def renomear_imagem_para_padrao(df, document_id, novo_nome_arquivo):\n",
    "    # registro atual\n",
    "    row = df.loc[df[\"document_id\"] == document_id].iloc[0]\n",
    "    path_antigo_rel = row[\"path_arquivo\"]\n",
    "    path_antigo = DATASET_PATH / path_antigo_rel\n",
    "\n",
    "    if not path_antigo.exists():\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado: {path_antigo}\")\n",
    "\n",
    "    novo_nome = novo_nome_arquivo.lower()\n",
    "    path_novo = IMAGENS_DIR / novo_nome\n",
    "    path_novo_rel = f\"imagens/{novo_nome}\"\n",
    "\n",
    "    if path_novo.exists():\n",
    "        raise FileExistsError(f\"Já existe um arquivo com esse nome: {path_novo.name}\")\n",
    "\n",
    "    # renomeia no disco\n",
    "    path_antigo.rename(path_novo)\n",
    "\n",
    "    # atualiza o CSV (df)\n",
    "    df.loc[df[\"document_id\"] == document_id, \"path_arquivo\"] = path_novo_rel\n",
    "\n",
    "    return {\n",
    "        \"document_id\": document_id,\n",
    "        \"campo\": \"path_arquivo\",\n",
    "        \"valor_antigo\": path_antigo_rel,\n",
    "        \"valor_novo\": path_novo_rel,\n",
    "        \"arquivo_existe_depois\": path_novo.exists(),\n",
    "        \"observacao\": \"Padronização do nome do arquivo de imagem (IMG_###_nome_receita.jpg)\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_imgs = []\n",
    "\n",
    "logs_imgs.append(\n",
    "    renomear_imagem_para_padrao(df, \"IMG_012\", \"img_012_rabada.jpg\")\n",
    ")\n",
    "\n",
    "logs_imgs.append(\n",
    "    renomear_imagem_para_padrao(df, \"IMG_022\", \"img_022_cocada_cremosa.jpg\")\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(logs_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"document_id\"].isin([\"IMG_012\", \"IMG_022\"]),\n",
    "       [\"document_id\", \"path_arquivo\"]].assign(\n",
    "    arquivo_existe=lambda x: x[\"path_arquivo\"].apply(existe)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b354c3f",
   "metadata": {},
   "source": [
    "ETAPA 3 — Correção e remoção dos arquivos Duplicados (PDF_006 e PDF_016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee87ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"is_duplicate\"] == True,\n",
    "       [\"document_id\", \"path_arquivo\", \"duplicate_group\", \"versao_rag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# máscara para duplicados que NÃO queremos manter\n",
    "mask_remover = (df[\"is_duplicate\"] == True) & (df[\"versao_rag\"] != \"v1\")\n",
    "\n",
    "df_removidos = df[mask_remover].copy()\n",
    "df_mantidos = df[~mask_remover].copy()\n",
    "\n",
    "print(\"Registros removidos:\", len(df_removidos))\n",
    "print(\"Registros mantidos:\", len(df_mantidos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee948e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_remocoes = df_removidos[[\n",
    "    \"document_id\",\n",
    "    \"path_arquivo\",\n",
    "    \"duplicate_group\",\n",
    "    \"versao_rag\",\n",
    "    \"duplicate_policy\"\n",
    "]].copy()\n",
    "\n",
    "log_remocoes[\"acao\"] = \"removido_duplicado\"\n",
    "log_remocoes[\"observacao\"] = \"Duplicado intencional removido para normalização do dataset final\"\n",
    "\n",
    "log_remocoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_mantidos.copy()\n",
    "\n",
    "# opcional: limpar colunas que não fazem mais sentido\n",
    "df[\"versao_rag\"] = \"v1\"\n",
    "df[\"is_duplicate\"] = False\n",
    "df[\"duplicate_policy\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"document_id\"].isin([\"PDF_006\", \"PDF_016\"]),\n",
    "       [\"document_id\", \"path_arquivo\", \"versao_rag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inventário final curado salvo em: c:\\Users\\BlueShift\\Desktop\\atividade6\\dataset_restaurante\\inventario_curado.csv\n"
     ]
    }
   ],
   "source": [
    "OUT_INVENTARIO_CURADO = DATASET_PATH / \"inventario_curado.csv\"\n",
    "df.to_csv(OUT_INVENTARIO_CURADO, index=False)\n",
    "\n",
    "print(\"✅ Inventário final curado salvo em:\", OUT_INVENTARIO_CURADO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf600860",
   "metadata": {},
   "source": [
    "# Parte 2 — Preparação do Dataset para RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Nesta etapa, o objetivo foi transformar o inventário curado de documentos em uma base\n",
    "estruturada e adequada para uso em um sistema de Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "O foco desta fase não é ainda a interface do chatbot, mas sim garantir que o conteúdo\n",
    "esteja corretamente extraído, normalizado e segmentado, permitindo recuperação eficiente\n",
    "e respostas precisas em etapas posteriores.\n",
    "\n",
    "## 2.1 Extração de Conteúdo Textual\n",
    "\n",
    "Foram aplicadas técnicas distintas de extração conforme o tipo de documento:\n",
    "\n",
    "- **PDFs (fichas técnicas)**:  \n",
    "  O texto foi extraído diretamente dos arquivos PDF, preservando informações essenciais\n",
    "  como ingredientes, modo de preparo, tempo, categoria e observações técnicas.\n",
    "\n",
    "- **Imagens (JPEG)**:  \n",
    "  Foi configurado OCR via Tesseract. No entanto, como as imagens representam apenas fotos\n",
    "  ilustrativas dos pratos (sem texto embutido), a extração resultou em conteúdo vazio,\n",
    "  comportamento esperado e documentado.\n",
    "\n",
    "## 2.2 Limpeza e Normalização do Texto\n",
    "\n",
    "Após a extração, os textos passaram por um processo de limpeza e normalização, incluindo:\n",
    "- Remoção de caracteres inválidos e ruídos comuns de OCR\n",
    "- Padronização de espaços e quebras de linha\n",
    "- Garantia de consistência textual para posterior geração de embeddings\n",
    "\n",
    "Esse passo é fundamental para melhorar a qualidade da recuperação semântica.\n",
    "\n",
    "## 2.3 Segmentação em Chunks\n",
    "\n",
    "Os textos normalizados foram segmentados em **chunks semânticos**, respeitando:\n",
    "- Tamanho máximo por chunk\n",
    "- Sobreposição (overlap) para preservação de contexto\n",
    "\n",
    "Características observadas:\n",
    "- Cada ficha técnica (PDF) gerou, em média, **1 chunk**\n",
    "- Documentos ligeiramente maiores geraram **2 chunks**\n",
    "- Imagens não geraram chunks, por não conterem texto\n",
    "\n",
    "Ao final, foram gerados **28 chunks**, número coerente com o tamanho e a natureza do dataset.\n",
    "\n",
    "## 2.4 Dataset Final para RAG\n",
    "\n",
    "O resultado desta etapa é o arquivo:\n",
    "\n",
    "**`rag_dataset_chunks.csv`**\n",
    "\n",
    "Este dataset contém:\n",
    "- Identificação do documento (`document_id`)\n",
    "- Identificação do chunk (`chunk_id`)\n",
    "- Conteúdo textual do chunk\n",
    "- Metadados relevantes (categoria, origem, caminho do arquivo, etc.)\n",
    "\n",
    "Esse arquivo representa a **base final de conhecimento** que será utilizada nas próximas\n",
    "etapas de indexação vetorial, recuperação de contexto e geração de respostas com modelos\n",
    "de linguagem (LLMs).\n",
    "\n",
    "Com o dataset preparado, o projeto segue para a fase de **integração com LLM**, incluindo\n",
    "indexação por embeddings, recuperação semântica e construção do chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4b7b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df carregado do inventário curado\n",
      "Linhas: 52\n",
      "Colunas: ['document_id', 'tipo', 'path_arquivo', 'titulo', 'origem', 'data', 'categoria', 'versao', 'nivel_confianca', 'is_duplicate', 'duplicate_group', 'versao_rag', 'duplicate_policy']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>titulo</th>\n",
       "      <th>origem</th>\n",
       "      <th>data</th>\n",
       "      <th>categoria</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>duplicate_group</th>\n",
       "      <th>versao_rag</th>\n",
       "      <th>duplicate_policy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "      <td>Favada</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-12-04</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_003</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_03_feijao_de_corda.pdf</td>\n",
       "      <td>Feijao-de-Corda</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id tipo                                  path_arquivo  \\\n",
       "0     PDF_001  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf   \n",
       "1     PDF_002  pdf           fichas_tecnicas/ficha_02_favada.pdf   \n",
       "2     PDF_003  pdf  fichas_tecnicas/ficha_03_feijao_de_corda.pdf   \n",
       "\n",
       "            titulo                 origem        data     categoria versao  \\\n",
       "0    Baiao-de-Dois  Ficha técnica oficial  2025-02-05  Tradicionais   v1.0   \n",
       "1           Favada  Ficha técnica oficial  2025-12-04  Tradicionais   v1.0   \n",
       "2  Feijao-de-Corda  Ficha técnica oficial  2025-05-31  Tradicionais   v1.0   \n",
       "\n",
       "  nivel_confianca  is_duplicate duplicate_group versao_rag  duplicate_policy  \n",
       "0            alto         False             NaN         v1               NaN  \n",
       "1            alto         False             NaN         v1               NaN  \n",
       "2            alto         False             NaN         v1               NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# caminhos\n",
    "BASE_PATH = Path(r\"c:\\Users\\BlueShift\\Desktop\\atividade6\")\n",
    "DATASET_PATH = BASE_PATH / \"dataset_restaurante\"\n",
    "\n",
    "# sempre usar o inventário já curado\n",
    "CSV_CURADO = DATASET_PATH / \"inventario_curado.csv\"\n",
    "assert CSV_CURADO.exists(), f\"Não achei {CSV_CURADO}\"\n",
    "\n",
    "df = pd.read_csv(CSV_CURADO)\n",
    "\n",
    "print(\"✅ df carregado do inventário curado\")\n",
    "print(\"Linhas:\", len(df))\n",
    "print(\"Colunas:\", list(df.columns))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18e7dc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id                                path_arquivo  \\\n",
       "0     PDF_001  fichas_tecnicas/ficha_01_baiao_de_dois.pdf   \n",
       "1     PDF_002         fichas_tecnicas/ficha_02_favada.pdf   \n",
       "\n",
       "                                               texto  \n",
       "0  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...  \n",
       "1  FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extrair_texto_pdf(path_rel: str) -> str:\n",
    "    path = DATASET_PATH / path_rel\n",
    "    textos = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            textos.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(textos).strip()\n",
    "\n",
    "mask_pdf = df[\"tipo\"].str.lower().eq(\"pdf\")\n",
    "df.loc[mask_pdf, \"texto\"] = df.loc[mask_pdf, \"path_arquivo\"].apply(extrair_texto_pdf)\n",
    "\n",
    "df.loc[mask_pdf, [\"document_id\", \"path_arquivo\", \"texto\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbcb96b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IMG_001</td>\n",
       "      <td>imagens/img_01_baiao_de_dois.jpg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IMG_002</td>\n",
       "      <td>imagens/img_02_favada.jpg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                      path_arquivo texto\n",
       "26     IMG_001  imagens/img_01_baiao_de_dois.jpg      \n",
       "27     IMG_002         imagens/img_02_favada.jpg      "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def extrair_texto_imagem(path_rel: str) -> str:\n",
    "    path = DATASET_PATH / path_rel\n",
    "    img = Image.open(path)\n",
    "    return pytesseract.image_to_string(img, lang=\"por\").strip()\n",
    "\n",
    "mask_img = df[\"tipo\"].str.lower().eq(\"imagem\")\n",
    "df.loc[mask_img, \"texto\"] = df.loc[mask_img, \"path_arquivo\"].apply(extrair_texto_imagem)\n",
    "\n",
    "df.loc[mask_img, [\"document_id\", \"path_arquivo\", \"texto\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "207b7f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>texto_limpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id tipo                                        texto_limpo\n",
       "0     PDF_001  pdf  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...\n",
       "1     PDF_002  pdf  FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def limpar_texto(txt: str) -> str:\n",
    "    if txt is None:\n",
    "        return \"\"\n",
    "    txt = str(txt)\n",
    "    txt = txt.replace(\"\\x0c\", \" \")  # lixo comum do OCR\n",
    "    txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "df[\"texto_limpo\"] = df[\"texto\"].apply(limpar_texto)\n",
    "\n",
    "df[[\"document_id\", \"tipo\", \"texto_limpo\"]].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ee195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>IMG_005</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>IMG_006</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IMG_002</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IMG_001</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>IMG_004</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IMG_003</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>IMG_016</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>IMG_015</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>IMG_014</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IMG_013</td>\n",
       "      <td>imagem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id    tipo  n_chars\n",
       "30     IMG_005  imagem        3\n",
       "31     IMG_006  imagem        3\n",
       "27     IMG_002  imagem        3\n",
       "26     IMG_001  imagem        3\n",
       "29     IMG_004  imagem        3\n",
       "28     IMG_003  imagem        3\n",
       "41     IMG_016  imagem        3\n",
       "40     IMG_015  imagem        3\n",
       "39     IMG_014  imagem        3\n",
       "38     IMG_013  imagem        3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"n_chars\"] = df[\"texto_limpo\"].str.len()\n",
    "df[[\"document_id\", \"tipo\", \"n_chars\"]].sort_values(\"n_chars\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61447856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>qtd_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDF_004</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_003</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PDF_006</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDF_005</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PDF_007</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PDF_008</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PDF_010</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PDF_009</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PDF_013</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PDF_014</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PDF_011</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PDF_012</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PDF_015</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PDF_016</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PDF_018</td>\n",
       "      <td>pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id tipo  qtd_chunks\n",
       "3      PDF_004  pdf           1\n",
       "2      PDF_003  pdf           1\n",
       "5      PDF_006  pdf           1\n",
       "4      PDF_005  pdf           1\n",
       "6      PDF_007  pdf           1\n",
       "7      PDF_008  pdf           1\n",
       "9      PDF_010  pdf           1\n",
       "8      PDF_009  pdf           1\n",
       "12     PDF_013  pdf           1\n",
       "13     PDF_014  pdf           1\n",
       "10     PDF_011  pdf           1\n",
       "11     PDF_012  pdf           1\n",
       "14     PDF_015  pdf           1\n",
       "15     PDF_016  pdf           1\n",
       "17     PDF_018  pdf           1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_text(texto: str, chunk_size: int = 800, overlap: int = 150):\n",
    "    if not texto or len(texto.strip()) == 0:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(texto)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = texto[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if end == n:\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "df[\"chunks\"] = df[\"texto_limpo\"].apply(chunk_text)\n",
    "df[[\"document_id\", \"tipo\"]].assign(qtd_chunks=df[\"chunks\"].apply(len)).sort_values(\"qtd_chunks\").head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bc08e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunks</th>\n",
       "      <th>categoria</th>\n",
       "      <th>origem</th>\n",
       "      <th>titulo</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>2</td>\n",
       "      <td>ntra por último, sendo misturado com\\ncuidado ...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Favada</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>2</td>\n",
       "      <td>PREPARO\\n1h30\\nRESTRIÇÕES ALIMENTARES\\nSem glú...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Favada</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDF_003</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFeijão-de-Corda\\n...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Feijao-de-Corda</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_03_feijao_de_corda.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id  chunk_id                                             chunks  \\\n",
       "0     PDF_001         1  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...   \n",
       "1     PDF_001         2  ntra por último, sendo misturado com\\ncuidado ...   \n",
       "2     PDF_002         1  FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...   \n",
       "3     PDF_002         2  PREPARO\\n1h30\\nRESTRIÇÕES ALIMENTARES\\nSem glú...   \n",
       "4     PDF_003         1  FICHA TÉCNICA - RESTAURANTE\\nFeijão-de-Corda\\n...   \n",
       "\n",
       "      categoria                 origem           titulo versao  \\\n",
       "0  Tradicionais  Ficha técnica oficial    Baiao-de-Dois   v1.0   \n",
       "1  Tradicionais  Ficha técnica oficial    Baiao-de-Dois   v1.0   \n",
       "2  Tradicionais  Ficha técnica oficial           Favada   v1.0   \n",
       "3  Tradicionais  Ficha técnica oficial           Favada   v1.0   \n",
       "4  Tradicionais  Ficha técnica oficial  Feijao-de-Corda   v1.0   \n",
       "\n",
       "  nivel_confianca tipo                                  path_arquivo  \n",
       "0            alto  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf  \n",
       "1            alto  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf  \n",
       "2            alto  pdf           fichas_tecnicas/ficha_02_favada.pdf  \n",
       "3            alto  pdf           fichas_tecnicas/ficha_02_favada.pdf  \n",
       "4            alto  pdf  fichas_tecnicas/ficha_03_feijao_de_corda.pdf  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks = df.explode(\"chunks\").reset_index(drop=True)\n",
    "df_chunks = df_chunks[df_chunks[\"chunks\"].notna() & (df_chunks[\"chunks\"].str.len() > 0)].copy()\n",
    "\n",
    "df_chunks[\"chunk_id\"] = df_chunks.groupby(\"document_id\").cumcount().add(1)\n",
    "\n",
    "# metadados úteis (mantém só os que existirem no seu df)\n",
    "meta_cols = [c for c in [\"document_id\", \"chunk_id\", \"chunks\", \"categoria\", \"origem\", \"titulo\", \"versao\", \"nivel_confianca\", \"tipo\", \"path_arquivo\"] if c in df_chunks.columns]\n",
    "rag_dataset = df_chunks[meta_cols].copy()\n",
    "\n",
    "rag_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "279ed7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset RAG (chunks) salvo em: c:\\Users\\BlueShift\\Desktop\\atividade6\\dataset_restaurante\\rag_dataset_chunks.csv\n",
      "Total de chunks: 54\n"
     ]
    }
   ],
   "source": [
    "OUT_RAG_CHUNKS = DATASET_PATH / \"rag_dataset_chunks.csv\"\n",
    "rag_dataset.to_csv(OUT_RAG_CHUNKS, index=False)\n",
    "\n",
    "print(\"✅ Dataset RAG (chunks) salvo em:\", OUT_RAG_CHUNKS)\n",
    "print(\"Total de chunks:\", len(rag_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0165e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo RAG\n",
      "Docs no inventário: 52\n",
      "PDFs: 26\n",
      "Imagens: 26\n",
      "Chunks gerados: 54\n",
      "Média chunks por doc: 1.04\n"
     ]
    }
   ],
   "source": [
    "print(\"Resumo RAG\")\n",
    "print(\"Docs no inventário:\", len(df))\n",
    "print(\"PDFs:\", (df[\"tipo\"].str.lower() == \"pdf\").sum())\n",
    "print(\"Imagens:\", (df[\"tipo\"].str.lower() == \"imagem\").sum())\n",
    "print(\"Chunks gerados:\", len(rag_dataset))\n",
    "print(\"Média chunks por doc:\", round(len(rag_dataset) / len(df), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653df56f",
   "metadata": {},
   "source": [
    "# Parte 3 — Integração com LLM (Azure) e Chatbot RAG\n",
    "\n",
    "Nesta parte, transformamos o dataset segmentado em um chatbot funcional com RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "Até aqui já concluímos:\n",
    "- Curadoria do inventário e correção de inconsistências (paths, nomes e duplicados)\n",
    "- Extração de texto dos PDFs e organização do conteúdo\n",
    "- Segmentação do texto em chunks e geração do arquivo `rag_dataset_chunks.csv` (base final para recuperação)\n",
    "\n",
    "A partir deste ponto, entramos na fase de **LLM + Recuperação**, que envolve:\n",
    "\n",
    "## 3.1 Preparação de credenciais (Azure Key Vault)\n",
    "Objetivo:\n",
    "- Conectar no Azure Key Vault para recuperar com segurança as credenciais necessárias (sem expor chaves no código)\n",
    "- Carregar variáveis essenciais para o uso de embeddings e geração de resposta (LLM)\n",
    "\n",
    "Entregáveis dessa etapa:\n",
    "- Conexão validada com o Key Vault\n",
    "- Secrets carregados no runtime (ex.: endpoint e key do Azure OpenAI ou chave de API)\n",
    "\n",
    "## 3.2 Indexação Vetorial (VectorStore)\n",
    "\n",
    "Objetivo:\n",
    "- Criar um índice vetorial (VectorStore) a partir dos chunks gerados em `rag_dataset_chunks.csv`\n",
    "- Permitir recuperação eficiente por similaridade\n",
    "\n",
    "Implementação adotada:\n",
    "- **Embeddings locais (SentenceTransformers)** para vetorização dos chunks\n",
    "- **FAISS** como VectorStore (índice vetorial local)\n",
    "\n",
    "Observação:\n",
    "- A geração final de respostas (LLM) permanece no **Azure OpenAI** (via Key Vault),\n",
    "  garantindo padronização e uso de credenciais corporativas.\n",
    "\n",
    "## 3.3 Recuperação (Retrieve)\n",
    "Objetivo:\n",
    "- Dada uma pergunta do usuário, calcular o embedding da pergunta\n",
    "- Buscar os Top-k chunks mais relevantes com base em similaridade (cosseno)\n",
    "\n",
    "Resultado esperado:\n",
    "- Lista de chunks relevantes + metadados (document_id, chunk_id, path_arquivo, score)\n",
    "\n",
    "## 3.4 Geração de Resposta (Generate)\n",
    "Objetivo:\n",
    "- Montar um prompt com a pergunta + contexto recuperado (chunks)\n",
    "- Solicitar a resposta ao modelo de linguagem (LLM), mantendo a resposta ancorada nos documentos recuperados\n",
    "\n",
    "Resultado esperado:\n",
    "- Resposta final + rastreabilidade\n",
    "\n",
    "## 3.5 Chatbot (Loop de Perguntas) + Fontes\n",
    "Objetivo:\n",
    "- Criar um fluxo de chat simples (no notebook) ou interface futura (ex.: Streamlit)\n",
    "- Exibir a resposta e também as fontes utilizadas (documentos e chunks)\n",
    "\n",
    "Resultado esperado:\n",
    "- Chatbot funcional com RAG e transparência de fontes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "caf1ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: c:\\Users\\BlueShift\\Desktop\\atividade6\n",
      "DATASET_PATH existe? True\n",
      "Conteúdo: ['fichas_tecnicas', 'imagens', 'inventario_curado.csv', 'inventario_dataset.csv', 'rag_dataset_chunks.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "BASE_PATH = Path(r\"c:\\Users\\BlueShift\\Desktop\\atividade6\")\n",
    "DATASET_PATH = BASE_PATH / \"dataset_restaurante\"\n",
    "\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "print(\"DATASET_PATH existe?\", DATASET_PATH.exists())\n",
    "print(\"Conteúdo:\", [p.name for p in DATASET_PATH.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe424f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado ao Key Vault: https://kv-academy-01.vault.azure.net\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "KEY_VAULT_NAME = \"kv-academy-01\"\n",
    "KV_URI = f\"https://kv-academy-01.vault.azure.net\"\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "kv_client = SecretClient(vault_url=KV_URI, credential=credential)\n",
    "\n",
    "print(\"✅ Conectado ao Key Vault:\", KV_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb68cb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Azure OpenAI carregado do Key Vault:\n",
      "Endpoint: https://oai-academy-ia.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview\n",
      "API Version: 2024-12-01-preview\n",
      "Chat deployment: gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "SECRET_ENDPOINT    = \"URL-API-GPT\"\n",
    "SECRET_API_VERSION = \"VERSION-API-GPT\"\n",
    "SECRET_API_KEY     = \"KEY-API-GPT\"     \n",
    "SECRET_DEPLOY_CHAT = \"MODELO-APT-GPT\"\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT     = kv_client.get_secret(SECRET_ENDPOINT).value\n",
    "AZURE_OPENAI_API_VERSION  = kv_client.get_secret(SECRET_API_VERSION).value\n",
    "AZURE_OPENAI_API_KEY      = kv_client.get_secret(SECRET_API_KEY).value\n",
    "AZURE_OPENAI_CHAT_DEPLOY  = kv_client.get_secret(SECRET_DEPLOY_CHAT).value\n",
    "\n",
    "print(\"✅ Azure OpenAI carregado do Key Vault:\")\n",
    "print(\"Endpoint:\", AZURE_OPENAI_ENDPOINT)\n",
    "print(\"API Version:\", AZURE_OPENAI_API_VERSION)\n",
    "print(\"Chat deployment:\", AZURE_OPENAI_CHAT_DEPLOY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adb5d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Teste chat: OK\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_CHAT_DEPLOY,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Responda apenas com OK.\"}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"✅ Teste chat:\", resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44d9679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunks carregados: 54\n",
      "Colunas: ['document_id', 'chunk_id', 'chunks', 'categoria', 'origem', 'titulo', 'versao', 'nivel_confianca', 'tipo', 'path_arquivo']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunks</th>\n",
       "      <th>categoria</th>\n",
       "      <th>origem</th>\n",
       "      <th>titulo</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>2</td>\n",
       "      <td>ntra por último, sendo misturado com\\ncuidado ...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id  chunk_id                                             chunks  \\\n",
       "0     PDF_001         1  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...   \n",
       "1     PDF_001         2  ntra por último, sendo misturado com\\ncuidado ...   \n",
       "\n",
       "      categoria                 origem         titulo versao nivel_confianca  \\\n",
       "0  Tradicionais  Ficha técnica oficial  Baiao-de-Dois   v1.0            alto   \n",
       "1  Tradicionais  Ficha técnica oficial  Baiao-de-Dois   v1.0            alto   \n",
       "\n",
       "  tipo                                path_arquivo  \n",
       "0  pdf  fichas_tecnicas/ficha_01_baiao_de_dois.pdf  \n",
       "1  pdf  fichas_tecnicas/ficha_01_baiao_de_dois.pdf  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_CHUNKS_PATH = DATASET_PATH / \"rag_dataset_chunks.csv\"\n",
    "assert RAG_CHUNKS_PATH.exists(), f\"Não encontrei: {RAG_CHUNKS_PATH}\"\n",
    "\n",
    "rag_dataset = pd.read_csv(RAG_CHUNKS_PATH)\n",
    "\n",
    "print(\"✅ Chunks carregados:\", len(rag_dataset))\n",
    "print(\"Colunas:\", list(rag_dataset.columns))\n",
    "rag_dataset.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4236843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = rag_dataset[\"chunks\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "E = model_st.encode(\n",
    "    texts,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "print(\"✅ Embeddings locais gerados:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6830d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VectorStore FAISS criado. Vetores: 54\n"
     ]
    }
   ],
   "source": [
    "dim = E.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)   # IP + normalizado => cosseno\n",
    "index.add(E)\n",
    "\n",
    "print(\"✅ VectorStore FAISS criado. Vetores:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eae90f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_faiss(query: str, top_k: int = 10):\n",
    "    q = model_st.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, idx = index.search(q, top_k)\n",
    "\n",
    "    hits = rag_dataset.iloc[idx[0]].copy()\n",
    "    hits[\"score\"] = scores[0]\n",
    "    return hits.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d03b3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(rows, max_chars=4500):\n",
    "    parts, total = [], 0\n",
    "    for r in rows.itertuples():\n",
    "        tag = f\"[Fonte: {r.document_id} | chunk {r.chunk_id} | {r.path_arquivo}]\"\n",
    "        block = f\"{tag}\\n{str(r.chunks).strip()}\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def generate_answer(query: str, top_k: int = 5, temperature: float = 0.2, min_score: float = 0.28):\n",
    "    hits = retrieve_faiss(query, top_k=max(top_k, 20))\n",
    "\n",
    "    if min_score is not None:\n",
    "        hits = hits[hits[\"score\"] >= min_score]\n",
    "\n",
    "    # 1 chunk por doc (reduz poluição)\n",
    "    hits = hits.drop_duplicates(subset=[\"document_id\"]).head(top_k).copy()\n",
    "\n",
    "    context = format_context(hits)\n",
    "\n",
    "    system = (\n",
    "        \"Você é um assistente de um restaurante. \"\n",
    "        \"Responda SOMENTE com base no CONTEXTO fornecido. \"\n",
    "        \"Se o contexto não tiver a informação, diga que não encontrou na base. \"\n",
    "        \"No final, liste as fontes usadas no formato: document_id (chunk_id).\"\n",
    "    )\n",
    "\n",
    "    user = f\"PERGUNTA:\\n{query}\\n\\nCONTEXTO:\\n{context}\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOY,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content, hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5a93fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O preço da Rabada está na faixa de R$ 60 a R$ 70.\n",
      "\n",
      "Fonte: PDF_012 (chunk 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>score</th>\n",
       "      <th>categoria_norm</th>\n",
       "      <th>titulo</th>\n",
       "      <th>path_arquivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PDF_012</td>\n",
       "      <td>1</td>\n",
       "      <td>0.411028</td>\n",
       "      <td>especialidade</td>\n",
       "      <td>Rabada</td>\n",
       "      <td>fichas_tecnicas/ficha_12_rabada.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id  chunk_id     score categoria_norm  titulo  \\\n",
       "13     PDF_012         1  0.411028  especialidade  Rabada   \n",
       "\n",
       "                           path_arquivo  \n",
       "13  fichas_tecnicas/ficha_12_rabada.pdf  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans, used = generate_answer(\"Qual preço da Rabada\", top_k=5)\n",
    "print(ans)\n",
    "\n",
    "used_cols = [c for c in [\"document_id\",\"chunk_id\",\"score\",\"categoria_norm\",\"titulo\",\"path_arquivo\"] if c in used.columns]\n",
    "used[used_cols].head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_list_question(q: str) -> bool:\n",
    "    q = q.lower()\n",
    "    gatilhos = [\"quais\", \"quais são\", \"liste\", \"me diga\", \"tipos de\", \"opções\", \"tem quais\", \"cardápio\", \"o que tem\"]\n",
    "    return any(g in q for g in gatilhos)\n",
    "\n",
    "def infer_category_from_question(q: str):\n",
    "    q = q.lower()\n",
    "    if \"salad\" in q: return \"salada\"\n",
    "    if \"sobrem\" in q or \"doce\" in q: return \"sobremesa\"\n",
    "    if \"especial\" in q or \"da casa\" in q: return \"especialidade\"\n",
    "    if \"trad\" in q: return \"tradicional\"\n",
    "    return None\n",
    "\n",
    "def retrieve_for_listing(query: str, top_k: int = 40, min_score: float = 0.23):\n",
    "    hits = retrieve_faiss(query, top_k=top_k)\n",
    "    hits = hits[hits[\"score\"] >= min_score]\n",
    "\n",
    "    cat = infer_category_from_question(query)\n",
    "    if cat is not None and \"categoria_norm\" in hits.columns:\n",
    "        hits_cat = hits[hits[\"categoria_norm\"] == cat]\n",
    "        if len(hits_cat) > 0:\n",
    "            hits = hits_cat\n",
    "\n",
    "    if \"titulo\" in hits.columns:\n",
    "        hits = hits.drop_duplicates(subset=[\"titulo\"])\n",
    "    else:\n",
    "        hits = hits.drop_duplicates(subset=[\"document_id\"])\n",
    "\n",
    "    return hits, cat\n",
    "\n",
    "def answer_listing(query: str, max_items: int = 10):\n",
    "    hits, cat = retrieve_for_listing(query)\n",
    "    name_col = \"titulo\" if \"titulo\" in hits.columns else \"document_id\"\n",
    "\n",
    "    items = hits[name_col].head(max_items).tolist()\n",
    "    if not items:\n",
    "        return \"Não encontrei itens suficientes na base.\", hits\n",
    "\n",
    "    header = f\"Itens encontrados{' na categoria ' + cat if cat else ''}:\"\n",
    "    body = \"\\n\".join([f\"- {x}\" for x in items])\n",
    "    return f\"{header}\\n{body}\", hits.head(max_items)\n",
    "\n",
    "def show_sources(df, n=1):\n",
    "    cols = [c for c in [\"document_id\",\"chunk_id\",\"score\",\"categoria_norm\",\"titulo\",\"path_arquivo\"] if c in df.columns]\n",
    "    if cols:\n",
    "        print(\"\\nFontes (top):\")\n",
    "        print(df[cols].head(n).to_string(index=False))\n",
    "\n",
    "while True:\n",
    "    q = input(\"\\nPergunta (ENTER vazio para sair): \").strip()\n",
    "    if not q:\n",
    "        break\n",
    "\n",
    "    if is_list_question(q) and infer_category_from_question(q) is not None:\n",
    "        ans, src = answer_listing(q, max_items=10)\n",
    "        print(\"\\n\" + ans)\n",
    "        show_sources(src, n=10)\n",
    "        continue\n",
    "\n",
    "    ans, src = generate_answer(q, top_k=5, min_score=0.28)\n",
    "    print(\"\\nResposta:\\n\" + ans)\n",
    "    show_sources(src, n=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
