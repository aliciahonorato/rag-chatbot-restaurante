{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63227b0",
   "metadata": {},
   "source": [
    "# Notebook 2 — Preparação do Dataset para RAG\n",
    "\n",
    "Este notebook aplica as correções e padronizações necessárias para deixar o dataset pronto para uso em um pipeline de RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "Nesta etapa:\n",
    "- Corrigimos registros com inventário incorreto (ex.: PDF apontando para path inválido).\n",
    "- Padronizamos campos textuais e metadados.\n",
    "- Tratamos duplicados (mantendo os intencionais, com versionamento).\n",
    "- Geramos o arquivo final `rag_dataset.csv`.\n",
    "- Registramos todas as correções em logs para rastreabilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4726b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH existe? True\n",
      "CSV existe? True\n",
      "Pastas: ['fichas_tecnicas', 'imagens', 'inventario_curado.csv', 'inventario_dataset.csv', 'rag_dataset_chunks.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# notebook está em .../atividade6/notebooks\n",
    "BASE_PATH = Path(os.getcwd()).parent              # .../atividade6\n",
    "DATASET_PATH = BASE_PATH / \"dataset_restaurante\"  # .../atividade6/dataset_restaurante\n",
    "\n",
    "# Aqui é a \"raiz\" dos arquivos (PDFs e imagens)\n",
    "DATA_ROOT = DATASET_PATH\n",
    "\n",
    "CSV_PATH = DATASET_PATH / \"inventario_dataset.csv\"\n",
    "\n",
    "print(\"DATASET_PATH existe?\", DATASET_PATH.exists())\n",
    "print(\"CSV existe?\", CSV_PATH.exists())\n",
    "print(\"Pastas:\", [p.name for p in DATASET_PATH.iterdir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22ab9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas no inventário: 54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>titulo</th>\n",
       "      <th>origem</th>\n",
       "      <th>data</th>\n",
       "      <th>categoria</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "      <td>Favada</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-12-04</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_003</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_03_feijao_de_corda.pdf</td>\n",
       "      <td>Feijao-de-Corda</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDF_004</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_04_sarapatel.pdf</td>\n",
       "      <td>Sarapatel</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>15/03/2025</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDF_005</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_05_caldo_de_mocoto.pdf</td>\n",
       "      <td>Caldo de Mocoto</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>11-05-2025</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id tipo                                  path_arquivo  \\\n",
       "0     PDF_001  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf   \n",
       "1     PDF_002  pdf           fichas_tecnicas/ficha_02_favada.pdf   \n",
       "2     PDF_003  pdf  fichas_tecnicas/ficha_03_feijao_de_corda.pdf   \n",
       "3     PDF_004  pdf        fichas_tecnicas/ficha_04_sarapatel.pdf   \n",
       "4     PDF_005  pdf  fichas_tecnicas/ficha_05_caldo_de_mocoto.pdf   \n",
       "\n",
       "            titulo                 origem        data     categoria versao  \\\n",
       "0    Baiao-de-Dois  Ficha técnica oficial  2025-02-05  Tradicionais   v1.0   \n",
       "1           Favada  Ficha técnica oficial  2025-12-04  Tradicionais   v1.0   \n",
       "2  Feijao-de-Corda  Ficha técnica oficial  2025-05-31  Tradicionais   v1.0   \n",
       "3        Sarapatel  Ficha técnica oficial  15/03/2025  Tradicionais   v1.0   \n",
       "4  Caldo de Mocoto  Ficha técnica oficial  11-05-2025  Tradicionais   v1.0   \n",
       "\n",
       "  nivel_confianca  \n",
       "0            alto  \n",
       "1            alto  \n",
       "2            alto  \n",
       "3            alto  \n",
       "4            alto  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Linhas no inventário:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efc2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def existe(path_rel: str) -> bool:\n",
    "    return (DATA_ROOT / str(path_rel)).exists()\n",
    "\n",
    "def normalizar_texto(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    x = str(x).strip()\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c09ecd",
   "metadata": {},
   "source": [
    "## Etapa 1 — Correção dirigida do PDF_021\n",
    "\n",
    "O documento `PDF_021` estava com tipo \"pdf\", porém apontando para um caminho incorreto (imagem inexistente).\n",
    "Aqui corrigimos o `path_arquivo` para a ficha técnica correta do **Creme brûlée de doce de leite** e registramos a alteração em log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a8577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>campo</th>\n",
       "      <th>valor_antigo</th>\n",
       "      <th>valor_novo</th>\n",
       "      <th>arquivo_existe_depois</th>\n",
       "      <th>observacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_021</td>\n",
       "      <td>path_arquivo</td>\n",
       "      <td>imagens/arquivo_nao_existe.jpg</td>\n",
       "      <td>fichas_tecnicas/ficha_21_creme_brulee_de_doce_...</td>\n",
       "      <td>True</td>\n",
       "      <td>Correção do inventário: PDF apontava para path...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id         campo                    valor_antigo  \\\n",
       "0     PDF_021  path_arquivo  imagens/arquivo_nao_existe.jpg   \n",
       "\n",
       "                                          valor_novo  arquivo_existe_depois  \\\n",
       "0  fichas_tecnicas/ficha_21_creme_brulee_de_doce_...                   True   \n",
       "\n",
       "                                          observacao  \n",
       "0  Correção do inventário: PDF apontava para path...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_CORRETO_PDF21 = \"fichas_tecnicas/ficha_21_creme_brulee_de_doce_de_leite.pdf\"\n",
    "\n",
    "# antes\n",
    "path_antigo = df.loc[df[\"document_id\"] == \"PDF_021\", \"path_arquivo\"].iloc[0]\n",
    "\n",
    "# aplica correção\n",
    "df.loc[df[\"document_id\"] == \"PDF_021\", \"path_arquivo\"] = PATH_CORRETO_PDF21\n",
    "df.loc[df[\"document_id\"] == \"PDF_021\", \"tipo\"] = \"pdf\"\n",
    "\n",
    "log_fix_pdf21 = pd.DataFrame([{\n",
    "    \"document_id\": \"PDF_021\",\n",
    "    \"campo\": \"path_arquivo\",\n",
    "    \"valor_antigo\": path_antigo,\n",
    "    \"valor_novo\": PATH_CORRETO_PDF21,\n",
    "    \"arquivo_existe_depois\": existe(PATH_CORRETO_PDF21),\n",
    "    \"observacao\": \"Correção do inventário: PDF apontava para path incorreto\"\n",
    "}])\n",
    "\n",
    "log_fix_pdf21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab2d1f",
   "metadata": {},
   "source": [
    "ETAPA 2 — Padronização definitiva das nomenclaturas das imagens IMG_012 e IMG_022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1fac22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>arquivo_existe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>IMG_012</td>\n",
       "      <td>imagem</td>\n",
       "      <td>imagens/IMG_12.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IMG_022</td>\n",
       "      <td>imagem</td>\n",
       "      <td>imagens/img_22pratofinal.jpg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id    tipo                  path_arquivo  arquivo_existe\n",
       "37     IMG_012  imagem            imagens/IMG_12.jpg           False\n",
       "47     IMG_022  imagem  imagens/img_22pratofinal.jpg           False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"document_id\"].isin([\"IMG_012\", \"IMG_022\"]),\n",
    "       [\"document_id\", \"tipo\", \"path_arquivo\"]].assign(\n",
    "    arquivo_existe=lambda x: x[\"path_arquivo\"].apply(existe)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa13533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGENS_DIR = DATASET_PATH / \"imagens\"\n",
    "\n",
    "def renomear_imagem_para_padrao(df, document_id, novo_nome_arquivo):\n",
    "    # registro atual\n",
    "    row = df.loc[df[\"document_id\"] == document_id].iloc[0]\n",
    "    path_antigo_rel = row[\"path_arquivo\"]\n",
    "    path_antigo = DATASET_PATH / path_antigo_rel\n",
    "\n",
    "    if not path_antigo.exists():\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado: {path_antigo}\")\n",
    "\n",
    "    novo_nome = novo_nome_arquivo.lower()\n",
    "    path_novo = IMAGENS_DIR / novo_nome\n",
    "    path_novo_rel = f\"imagens/{novo_nome}\"\n",
    "\n",
    "    if path_novo.exists():\n",
    "        raise FileExistsError(f\"Já existe um arquivo com esse nome: {path_novo.name}\")\n",
    "\n",
    "    # renomeia no disco\n",
    "    path_antigo.rename(path_novo)\n",
    "\n",
    "    # atualiza o CSV (df)\n",
    "    df.loc[df[\"document_id\"] == document_id, \"path_arquivo\"] = path_novo_rel\n",
    "\n",
    "    return {\n",
    "        \"document_id\": document_id,\n",
    "        \"campo\": \"path_arquivo\",\n",
    "        \"valor_antigo\": path_antigo_rel,\n",
    "        \"valor_novo\": path_novo_rel,\n",
    "        \"arquivo_existe_depois\": path_novo.exists(),\n",
    "        \"observacao\": \"Padronização do nome do arquivo de imagem (IMG_###_nome_receita.jpg)\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_imgs = []\n",
    "\n",
    "logs_imgs.append(\n",
    "    renomear_imagem_para_padrao(df, \"IMG_012\", \"img_012_rabada.jpg\")\n",
    ")\n",
    "\n",
    "logs_imgs.append(\n",
    "    renomear_imagem_para_padrao(df, \"IMG_022\", \"img_022_cocada_cremosa.jpg\")\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(logs_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"document_id\"].isin([\"IMG_012\", \"IMG_022\"]),\n",
    "       [\"document_id\", \"path_arquivo\"]].assign(\n",
    "    arquivo_existe=lambda x: x[\"path_arquivo\"].apply(existe)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b354c3f",
   "metadata": {},
   "source": [
    "ETAPA 3 — Correção e remoção dos arquivos Duplicados (PDF_006 e PDF_016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee87ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"is_duplicate\"] == True,\n",
    "       [\"document_id\", \"path_arquivo\", \"duplicate_group\", \"versao_rag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# máscara para duplicados que NÃO queremos manter\n",
    "mask_remover = (df[\"is_duplicate\"] == True) & (df[\"versao_rag\"] != \"v1\")\n",
    "\n",
    "df_removidos = df[mask_remover].copy()\n",
    "df_mantidos = df[~mask_remover].copy()\n",
    "\n",
    "print(\"Registros removidos:\", len(df_removidos))\n",
    "print(\"Registros mantidos:\", len(df_mantidos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee948e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_remocoes = df_removidos[[\n",
    "    \"document_id\",\n",
    "    \"path_arquivo\",\n",
    "    \"duplicate_group\",\n",
    "    \"versao_rag\",\n",
    "    \"duplicate_policy\"\n",
    "]].copy()\n",
    "\n",
    "log_remocoes[\"acao\"] = \"removido_duplicado\"\n",
    "log_remocoes[\"observacao\"] = \"Duplicado intencional removido para normalização do dataset final\"\n",
    "\n",
    "log_remocoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_mantidos.copy()\n",
    "\n",
    "# opcional: limpar colunas que não fazem mais sentido\n",
    "df[\"versao_rag\"] = \"v1\"\n",
    "df[\"is_duplicate\"] = False\n",
    "df[\"duplicate_policy\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"document_id\"].isin([\"PDF_006\", \"PDF_016\"]),\n",
    "       [\"document_id\", \"path_arquivo\", \"versao_rag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inventário final curado salvo em: c:\\Users\\BlueShift\\Desktop\\atividade6\\dataset_restaurante\\inventario_curado.csv\n"
     ]
    }
   ],
   "source": [
    "OUT_INVENTARIO_CURADO = DATASET_PATH / \"inventario_curado.csv\"\n",
    "df.to_csv(OUT_INVENTARIO_CURADO, index=False)\n",
    "\n",
    "print(\"✅ Inventário final curado salvo em:\", OUT_INVENTARIO_CURADO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf600860",
   "metadata": {},
   "source": [
    "# Parte 2 — Preparação do Dataset para RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Nesta etapa, o objetivo foi transformar o inventário curado de documentos em uma base\n",
    "estruturada e adequada para uso em um sistema de Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "O foco desta fase não é ainda a interface do chatbot, mas sim garantir que o conteúdo\n",
    "esteja corretamente extraído, normalizado e segmentado, permitindo recuperação eficiente\n",
    "e respostas precisas em etapas posteriores.\n",
    "\n",
    "## 2.1 Extração de Conteúdo Textual\n",
    "\n",
    "Foram aplicadas técnicas distintas de extração conforme o tipo de documento:\n",
    "\n",
    "- **PDFs (fichas técnicas)**:  \n",
    "  O texto foi extraído diretamente dos arquivos PDF, preservando informações essenciais\n",
    "  como ingredientes, modo de preparo, tempo, categoria e observações técnicas.\n",
    "\n",
    "- **Imagens (JPEG)**:  \n",
    "  Foi configurado OCR via Tesseract. No entanto, como as imagens representam apenas fotos\n",
    "  ilustrativas dos pratos (sem texto embutido), a extração resultou em conteúdo vazio,\n",
    "  comportamento esperado e documentado.\n",
    "\n",
    "## 2.2 Limpeza e Normalização do Texto\n",
    "\n",
    "Após a extração, os textos passaram por um processo de limpeza e normalização, incluindo:\n",
    "- Remoção de caracteres inválidos e ruídos comuns de OCR\n",
    "- Padronização de espaços e quebras de linha\n",
    "- Garantia de consistência textual para posterior geração de embeddings\n",
    "\n",
    "Esse passo é fundamental para melhorar a qualidade da recuperação semântica.\n",
    "\n",
    "## 2.3 Segmentação em Chunks\n",
    "\n",
    "Os textos normalizados foram segmentados em **chunks semânticos**, respeitando:\n",
    "- Tamanho máximo por chunk\n",
    "- Sobreposição (overlap) para preservação de contexto\n",
    "\n",
    "Características observadas:\n",
    "- Cada ficha técnica (PDF) gerou, em média, **1 chunk**\n",
    "- Documentos ligeiramente maiores geraram **2 chunks**\n",
    "- Imagens não geraram chunks, por não conterem texto\n",
    "\n",
    "Ao final, foram gerados **28 chunks**, número coerente com o tamanho e a natureza do dataset.\n",
    "\n",
    "## 2.4 Dataset Final para RAG\n",
    "\n",
    "O resultado desta etapa é o arquivo:\n",
    "\n",
    "**`rag_dataset_chunks.csv`**\n",
    "\n",
    "Este dataset contém:\n",
    "- Identificação do documento (`document_id`)\n",
    "- Identificação do chunk (`chunk_id`)\n",
    "- Conteúdo textual do chunk\n",
    "- Metadados relevantes (categoria, origem, caminho do arquivo, etc.)\n",
    "\n",
    "Esse arquivo representa a **base final de conhecimento** que será utilizada nas próximas\n",
    "etapas de indexação vetorial, recuperação de contexto e geração de respostas com modelos\n",
    "de linguagem (LLMs).\n",
    "\n",
    "Com o dataset preparado, o projeto segue para a fase de **integração com LLM**, incluindo\n",
    "indexação por embeddings, recuperação semântica e construção do chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b7b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df carregado do inventário curado\n",
      "Linhas: 52\n",
      "Colunas: ['document_id', 'tipo', 'path_arquivo', 'titulo', 'origem', 'data', 'categoria', 'versao', 'nivel_confianca', 'is_duplicate', 'duplicate_group', 'versao_rag', 'duplicate_policy']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>titulo</th>\n",
       "      <th>origem</th>\n",
       "      <th>data</th>\n",
       "      <th>categoria</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>duplicate_group</th>\n",
       "      <th>versao_rag</th>\n",
       "      <th>duplicate_policy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-02-05</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "      <td>Favada</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-12-04</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_003</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_03_feijao_de_corda.pdf</td>\n",
       "      <td>Feijao-de-Corda</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id tipo                                  path_arquivo  \\\n",
       "0     PDF_001  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf   \n",
       "1     PDF_002  pdf           fichas_tecnicas/ficha_02_favada.pdf   \n",
       "2     PDF_003  pdf  fichas_tecnicas/ficha_03_feijao_de_corda.pdf   \n",
       "\n",
       "            titulo                 origem        data     categoria versao  \\\n",
       "0    Baiao-de-Dois  Ficha técnica oficial  2025-02-05  Tradicionais   v1.0   \n",
       "1           Favada  Ficha técnica oficial  2025-12-04  Tradicionais   v1.0   \n",
       "2  Feijao-de-Corda  Ficha técnica oficial  2025-05-31  Tradicionais   v1.0   \n",
       "\n",
       "  nivel_confianca  is_duplicate duplicate_group versao_rag  duplicate_policy  \n",
       "0            alto         False             NaN         v1               NaN  \n",
       "1            alto         False             NaN         v1               NaN  \n",
       "2            alto         False             NaN         v1               NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# caminhos\n",
    "BASE_PATH = Path(r\"c:\\Users\\BlueShift\\Desktop\\atividade6\")\n",
    "DATASET_PATH = BASE_PATH / \"dataset_restaurante\"\n",
    "\n",
    "# sempre usar o inventário já curado\n",
    "CSV_CURADO = DATASET_PATH / \"inventario_curado.csv\"\n",
    "assert CSV_CURADO.exists(), f\"Não achei {CSV_CURADO}\"\n",
    "\n",
    "df = pd.read_csv(CSV_CURADO)\n",
    "\n",
    "print(\"✅ df carregado do inventário curado\")\n",
    "print(\"Linhas:\", len(df))\n",
    "print(\"Colunas:\", list(df.columns))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e7dc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id                                path_arquivo  \\\n",
       "0     PDF_001  fichas_tecnicas/ficha_01_baiao_de_dois.pdf   \n",
       "1     PDF_002         fichas_tecnicas/ficha_02_favada.pdf   \n",
       "\n",
       "                                               texto  \n",
       "0  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...  \n",
       "1  FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extrair_texto_pdf(path_rel: str) -> str:\n",
    "    path = DATASET_PATH / path_rel\n",
    "    textos = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            textos.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(textos).strip()\n",
    "\n",
    "mask_pdf = df[\"tipo\"].str.lower().eq(\"pdf\")\n",
    "df.loc[mask_pdf, \"texto\"] = df.loc[mask_pdf, \"path_arquivo\"].apply(extrair_texto_pdf)\n",
    "\n",
    "df.loc[mask_pdf, [\"document_id\", \"path_arquivo\", \"texto\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcb96b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>path_arquivo</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IMG_001</td>\n",
       "      <td>imagens/img_01_baiao_de_dois.jpg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IMG_002</td>\n",
       "      <td>imagens/img_02_favada.jpg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                      path_arquivo texto\n",
       "26     IMG_001  imagens/img_01_baiao_de_dois.jpg      \n",
       "27     IMG_002         imagens/img_02_favada.jpg      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def extrair_texto_imagem(path_rel: str) -> str:\n",
    "    path = DATASET_PATH / path_rel\n",
    "    img = Image.open(path)\n",
    "    return pytesseract.image_to_string(img, lang=\"por\").strip()\n",
    "\n",
    "mask_img = df[\"tipo\"].str.lower().eq(\"imagem\")\n",
    "df.loc[mask_img, \"texto\"] = df.loc[mask_img, \"path_arquivo\"].apply(extrair_texto_imagem)\n",
    "\n",
    "df.loc[mask_img, [\"document_id\", \"path_arquivo\", \"texto\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207b7f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>texto_limpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>pdf</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id tipo                                        texto_limpo\n",
       "0     PDF_001  pdf  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...\n",
       "1     PDF_002  pdf  FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def limpar_texto(txt: str) -> str:\n",
    "    if txt is None:\n",
    "        return \"\"\n",
    "    txt = str(txt)\n",
    "    txt = txt.replace(\"\\x0c\", \" \")  # lixo comum do OCR\n",
    "    txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "df[\"texto_limpo\"] = df[\"texto\"].apply(limpar_texto)\n",
    "\n",
    "df[[\"document_id\", \"tipo\", \"texto_limpo\"]].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ee195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>IMG_005</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>IMG_006</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IMG_002</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IMG_001</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>IMG_004</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IMG_003</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>IMG_016</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>IMG_015</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>IMG_014</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IMG_013</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id    tipo  n_chars\n",
       "30     IMG_005  imagem        0\n",
       "31     IMG_006  imagem        0\n",
       "27     IMG_002  imagem        0\n",
       "26     IMG_001  imagem        0\n",
       "29     IMG_004  imagem        0\n",
       "28     IMG_003  imagem        0\n",
       "41     IMG_016  imagem        0\n",
       "40     IMG_015  imagem        0\n",
       "39     IMG_014  imagem        0\n",
       "38     IMG_013  imagem        0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"n_chars\"] = df[\"texto_limpo\"].str.len()\n",
    "df[[\"document_id\", \"tipo\", \"n_chars\"]].sort_values(\"n_chars\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61447856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tipo</th>\n",
       "      <th>qtd_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>IMG_005</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>IMG_006</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IMG_002</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IMG_001</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>IMG_004</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IMG_003</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>IMG_016</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>IMG_015</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>IMG_014</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IMG_013</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>IMG_012</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>IMG_011</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>IMG_026</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>IMG_025</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>IMG_024</td>\n",
       "      <td>imagem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id    tipo  qtd_chunks\n",
       "30     IMG_005  imagem           0\n",
       "31     IMG_006  imagem           0\n",
       "27     IMG_002  imagem           0\n",
       "26     IMG_001  imagem           0\n",
       "29     IMG_004  imagem           0\n",
       "28     IMG_003  imagem           0\n",
       "41     IMG_016  imagem           0\n",
       "40     IMG_015  imagem           0\n",
       "39     IMG_014  imagem           0\n",
       "38     IMG_013  imagem           0\n",
       "37     IMG_012  imagem           0\n",
       "36     IMG_011  imagem           0\n",
       "51     IMG_026  imagem           0\n",
       "50     IMG_025  imagem           0\n",
       "49     IMG_024  imagem           0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_text(texto: str, chunk_size: int = 800, overlap: int = 150):\n",
    "    if not texto or len(texto.strip()) == 0:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(texto)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = texto[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if end == n:\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "df[\"chunks\"] = df[\"texto_limpo\"].apply(chunk_text)\n",
    "df[[\"document_id\", \"tipo\"]].assign(qtd_chunks=df[\"chunks\"].apply(len)).sort_values(\"qtd_chunks\").head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc08e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunks</th>\n",
       "      <th>categoria</th>\n",
       "      <th>origem</th>\n",
       "      <th>titulo</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>2</td>\n",
       "      <td>ntra por último, sendo misturado com\\ncuidado ...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Favada</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDF_002</td>\n",
       "      <td>2</td>\n",
       "      <td>PREPARO\\n1h30\\nRESTRIÇÕES ALIMENTARES\\nSem glú...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Favada</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_02_favada.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDF_003</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nFeijão-de-Corda\\n...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Feijao-de-Corda</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_03_feijao_de_corda.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id  chunk_id                                             chunks  \\\n",
       "0     PDF_001         1  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...   \n",
       "1     PDF_001         2  ntra por último, sendo misturado com\\ncuidado ...   \n",
       "2     PDF_002         1  FICHA TÉCNICA - RESTAURANTE\\nFavada\\nCATEGORIA...   \n",
       "3     PDF_002         2  PREPARO\\n1h30\\nRESTRIÇÕES ALIMENTARES\\nSem glú...   \n",
       "4     PDF_003         1  FICHA TÉCNICA - RESTAURANTE\\nFeijão-de-Corda\\n...   \n",
       "\n",
       "      categoria                 origem           titulo versao  \\\n",
       "0  Tradicionais  Ficha técnica oficial    Baiao-de-Dois   v1.0   \n",
       "1  Tradicionais  Ficha técnica oficial    Baiao-de-Dois   v1.0   \n",
       "2  Tradicionais  Ficha técnica oficial           Favada   v1.0   \n",
       "3  Tradicionais  Ficha técnica oficial           Favada   v1.0   \n",
       "4  Tradicionais  Ficha técnica oficial  Feijao-de-Corda   v1.0   \n",
       "\n",
       "  nivel_confianca tipo                                  path_arquivo  \n",
       "0            alto  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf  \n",
       "1            alto  pdf    fichas_tecnicas/ficha_01_baiao_de_dois.pdf  \n",
       "2            alto  pdf           fichas_tecnicas/ficha_02_favada.pdf  \n",
       "3            alto  pdf           fichas_tecnicas/ficha_02_favada.pdf  \n",
       "4            alto  pdf  fichas_tecnicas/ficha_03_feijao_de_corda.pdf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks = df.explode(\"chunks\").reset_index(drop=True)\n",
    "df_chunks = df_chunks[df_chunks[\"chunks\"].notna() & (df_chunks[\"chunks\"].str.len() > 0)].copy()\n",
    "\n",
    "df_chunks[\"chunk_id\"] = df_chunks.groupby(\"document_id\").cumcount().add(1)\n",
    "\n",
    "# metadados úteis (mantém só os que existirem no seu df)\n",
    "meta_cols = [c for c in [\"document_id\", \"chunk_id\", \"chunks\", \"categoria\", \"origem\", \"titulo\", \"versao\", \"nivel_confianca\", \"tipo\", \"path_arquivo\"] if c in df_chunks.columns]\n",
    "rag_dataset = df_chunks[meta_cols].copy()\n",
    "\n",
    "rag_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "279ed7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset RAG (chunks) salvo em: c:\\Users\\BlueShift\\Desktop\\atividade6\\dataset_restaurante\\rag_dataset_chunks.csv\n",
      "Total de chunks: 28\n"
     ]
    }
   ],
   "source": [
    "OUT_RAG_CHUNKS = DATASET_PATH / \"rag_dataset_chunks.csv\"\n",
    "rag_dataset.to_csv(OUT_RAG_CHUNKS, index=False)\n",
    "\n",
    "print(\"✅ Dataset RAG (chunks) salvo em:\", OUT_RAG_CHUNKS)\n",
    "print(\"Total de chunks:\", len(rag_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0165e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo RAG\n",
      "Docs no inventário: 52\n",
      "PDFs: 26\n",
      "Imagens: 26\n",
      "Chunks gerados: 28\n",
      "Média chunks por doc: 0.54\n"
     ]
    }
   ],
   "source": [
    "print(\"Resumo RAG\")\n",
    "print(\"Docs no inventário:\", len(df))\n",
    "print(\"PDFs:\", (df[\"tipo\"].str.lower() == \"pdf\").sum())\n",
    "print(\"Imagens:\", (df[\"tipo\"].str.lower() == \"imagem\").sum())\n",
    "print(\"Chunks gerados:\", len(rag_dataset))\n",
    "print(\"Média chunks por doc:\", round(len(rag_dataset) / len(df), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653df56f",
   "metadata": {},
   "source": [
    "# Parte 3 — Integração com LLM (Azure) e Chatbot RAG\n",
    "\n",
    "Nesta parte, transformamos o dataset segmentado em um chatbot funcional com RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "Até aqui já concluímos:\n",
    "- Curadoria do inventário e correção de inconsistências (paths, nomes e duplicados)\n",
    "- Extração de texto dos PDFs e organização do conteúdo\n",
    "- Segmentação do texto em chunks e geração do arquivo `rag_dataset_chunks.csv` (base final para recuperação)\n",
    "\n",
    "A partir deste ponto, entramos na fase de **LLM + Recuperação**, que envolve:\n",
    "\n",
    "## 3.1 Preparação de credenciais (Azure Key Vault)\n",
    "Objetivo:\n",
    "- Conectar no Azure Key Vault para recuperar com segurança as credenciais necessárias (sem expor chaves no código)\n",
    "- Carregar variáveis essenciais para o uso de embeddings e geração de resposta (LLM)\n",
    "\n",
    "Entregáveis dessa etapa:\n",
    "- Conexão validada com o Key Vault\n",
    "- Secrets carregados no runtime (ex.: endpoint e key do Azure OpenAI ou chave de API)\n",
    "\n",
    "## 3.2 Indexação Vetorial (VectorStore)\n",
    "\n",
    "Objetivo:\n",
    "- Criar um índice vetorial (VectorStore) a partir dos chunks gerados em `rag_dataset_chunks.csv`\n",
    "- Permitir recuperação eficiente por similaridade\n",
    "\n",
    "Implementação adotada:\n",
    "- **Embeddings locais (SentenceTransformers)** para vetorização dos chunks\n",
    "- **FAISS** como VectorStore (índice vetorial local)\n",
    "\n",
    "Observação:\n",
    "- A geração final de respostas (LLM) permanece no **Azure OpenAI** (via Key Vault),\n",
    "  garantindo padronização e uso de credenciais corporativas.\n",
    "\n",
    "## 3.3 Recuperação (Retrieve)\n",
    "Objetivo:\n",
    "- Dada uma pergunta do usuário, calcular o embedding da pergunta\n",
    "- Buscar os Top-k chunks mais relevantes com base em similaridade (cosseno)\n",
    "\n",
    "Resultado esperado:\n",
    "- Lista de chunks relevantes + metadados (document_id, chunk_id, path_arquivo, score)\n",
    "\n",
    "## 3.4 Geração de Resposta (Generate)\n",
    "Objetivo:\n",
    "- Montar um prompt com a pergunta + contexto recuperado (chunks)\n",
    "- Solicitar a resposta ao modelo de linguagem (LLM), mantendo a resposta ancorada nos documentos recuperados\n",
    "\n",
    "Resultado esperado:\n",
    "- Resposta final + rastreabilidade\n",
    "\n",
    "## 3.5 Chatbot (Loop de Perguntas) + Fontes\n",
    "Objetivo:\n",
    "- Criar um fluxo de chat simples (no notebook) ou interface futura (ex.: Streamlit)\n",
    "- Exibir a resposta e também as fontes utilizadas (documentos e chunks)\n",
    "\n",
    "Resultado esperado:\n",
    "- Chatbot funcional com RAG e transparência de fontes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf1ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: c:\\Users\\BlueShift\\Desktop\\atividade6\n",
      "DATASET_PATH existe? True\n",
      "Conteúdo: ['fichas_tecnicas', 'imagens', 'inventario_curado.csv', 'inventario_dataset.csv', 'rag_dataset_chunks.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "BASE_PATH = Path(r\"c:\\Users\\BlueShift\\Desktop\\atividade6\")\n",
    "DATASET_PATH = BASE_PATH / \"dataset_restaurante\"\n",
    "\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "print(\"DATASET_PATH existe?\", DATASET_PATH.exists())\n",
    "print(\"Conteúdo:\", [p.name for p in DATASET_PATH.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe424f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado ao Key Vault: https://kv-academy-01.vault.azure.net\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "KEY_VAULT_NAME = \"kv-academy-01\"\n",
    "KV_URI = f\"https://kv-academy-01.vault.azure.net\"\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "kv_client = SecretClient(vault_url=KV_URI, credential=credential)\n",
    "\n",
    "print(\"✅ Conectado ao Key Vault:\", KV_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb68cb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Azure OpenAI carregado do Key Vault:\n",
      "Endpoint: https://oai-academy-ia.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2025-01-01-preview\n",
      "API Version: 2024-12-01-preview\n",
      "Chat deployment: gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "SECRET_ENDPOINT    = \"URL-API-GPT\"\n",
    "SECRET_API_VERSION = \"VERSION-API-GPT\"\n",
    "SECRET_API_KEY     = \"KEY-API-GPT\"     \n",
    "SECRET_DEPLOY_CHAT = \"MODELO-APT-GPT\"\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT     = kv_client.get_secret(SECRET_ENDPOINT).value\n",
    "AZURE_OPENAI_API_VERSION  = kv_client.get_secret(SECRET_API_VERSION).value\n",
    "AZURE_OPENAI_API_KEY      = kv_client.get_secret(SECRET_API_KEY).value\n",
    "AZURE_OPENAI_CHAT_DEPLOY  = kv_client.get_secret(SECRET_DEPLOY_CHAT).value\n",
    "\n",
    "print(\"✅ Azure OpenAI carregado do Key Vault:\")\n",
    "print(\"Endpoint:\", AZURE_OPENAI_ENDPOINT)\n",
    "print(\"API Version:\", AZURE_OPENAI_API_VERSION)\n",
    "print(\"Chat deployment:\", AZURE_OPENAI_CHAT_DEPLOY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb5d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Teste chat: OK\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_CHAT_DEPLOY,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Responda apenas com OK.\"}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"✅ Teste chat:\", resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d9679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunks carregados: 28\n",
      "Colunas: ['document_id', 'chunk_id', 'chunks', 'categoria', 'origem', 'titulo', 'versao', 'nivel_confianca', 'tipo', 'path_arquivo']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunks</th>\n",
       "      <th>categoria</th>\n",
       "      <th>origem</th>\n",
       "      <th>titulo</th>\n",
       "      <th>versao</th>\n",
       "      <th>nivel_confianca</th>\n",
       "      <th>tipo</th>\n",
       "      <th>path_arquivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>1</td>\n",
       "      <td>FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_001</td>\n",
       "      <td>2</td>\n",
       "      <td>ntra por último, sendo misturado com\\ncuidado ...</td>\n",
       "      <td>Tradicionais</td>\n",
       "      <td>Ficha técnica oficial</td>\n",
       "      <td>Baiao-de-Dois</td>\n",
       "      <td>v1.0</td>\n",
       "      <td>alto</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fichas_tecnicas/ficha_01_baiao_de_dois.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id  chunk_id                                             chunks  \\\n",
       "0     PDF_001         1  FICHA TÉCNICA - RESTAURANTE\\nBaião-de-Dois\\nCA...   \n",
       "1     PDF_001         2  ntra por último, sendo misturado com\\ncuidado ...   \n",
       "\n",
       "      categoria                 origem         titulo versao nivel_confianca  \\\n",
       "0  Tradicionais  Ficha técnica oficial  Baiao-de-Dois   v1.0            alto   \n",
       "1  Tradicionais  Ficha técnica oficial  Baiao-de-Dois   v1.0            alto   \n",
       "\n",
       "  tipo                                path_arquivo  \n",
       "0  pdf  fichas_tecnicas/ficha_01_baiao_de_dois.pdf  \n",
       "1  pdf  fichas_tecnicas/ficha_01_baiao_de_dois.pdf  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_CHUNKS_PATH = DATASET_PATH / \"rag_dataset_chunks.csv\"\n",
    "assert RAG_CHUNKS_PATH.exists(), f\"Não encontrei: {RAG_CHUNKS_PATH}\"\n",
    "\n",
    "rag_dataset = pd.read_csv(RAG_CHUNKS_PATH)\n",
    "\n",
    "print(\"✅ Chunks carregados:\", len(rag_dataset))\n",
    "print(\"Colunas:\", list(rag_dataset.columns))\n",
    "rag_dataset.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ddca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias corrigidas (categoria_corr):\n",
      "categoria_corr\n",
      "Sobremesa        10\n",
      "Especialidade     8\n",
      "Tradicional       7\n",
      "Salada            3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "CATEGORIAS_OFICIAIS = [\"Tradicional\", \"Especialidade\", \"Salada\", \"Sobremesa\"]\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "\n",
    "def norm_cat(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = _strip_accents(s.strip().lower())\n",
    "\n",
    "    if \"tradicion\" in t: return \"Tradicional\"\n",
    "    if \"especial\" in t: return \"Especialidade\"\n",
    "    if \"salad\" in t: return \"Salada\"\n",
    "    if \"sobremes\" in t: return \"Sobremesa\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_cat_from_chunk(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Tenta pegar \"CATEGORIA: X\" ou \"CATEGORIA\\nX\"\n",
    "    m = re.search(r'(?i)\\bCATEGORIA\\b\\s*[:\\n]\\s*([^\\n\\r]+)', text)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "# 1) Normaliza a categoria que veio no CSV\n",
    "rag_dataset[\"categoria_norm\"] = rag_dataset[\"categoria\"].apply(norm_cat)\n",
    "\n",
    "# 2) Extrai a categoria escrita dentro do chunk (quando existir)\n",
    "rag_dataset[\"categoria_in_chunk\"] = rag_dataset[\"chunks\"].apply(extract_cat_from_chunk)\n",
    "rag_dataset[\"categoria_chunk_norm\"] = rag_dataset[\"categoria_in_chunk\"].apply(norm_cat)\n",
    "\n",
    "# 3) Categoria corrigida final: usa a do chunk se existir, senão a do CSV\n",
    "rag_dataset[\"categoria_corr\"] = rag_dataset.apply(\n",
    "    lambda r: r[\"categoria_chunk_norm\"] if r[\"categoria_chunk_norm\"] else r[\"categoria_norm\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Categorias corrigidas (categoria_corr):\")\n",
    "print(rag_dataset[\"categoria_corr\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb5e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_categorias():\n",
    "    # Sempre retorna as 4 categorias oficiais, na mesma ordem\n",
    "    return CATEGORIAS_OFICIAIS\n",
    "\n",
    "def extrair_categoria_da_pergunta(pergunta: str) -> str | None:\n",
    "    p = pergunta.lower()\n",
    "    if \"tradicion\" in p: return \"Tradicional\"\n",
    "    if \"especial\" in p: return \"Especialidade\"\n",
    "    if \"salad\" in p: return \"Salada\"\n",
    "    if \"sobremes\" in p: return \"Sobremesa\"\n",
    "    return None\n",
    "\n",
    "def listar_pratos_da_categoria(cat: str):\n",
    "    # Para listar pratos do cardápio, consideramos só os PDFs (receitas/pratos)\n",
    "    df_menu = rag_dataset[rag_dataset[\"tipo\"].astype(str).str.lower() == \"pdf\"].copy()\n",
    "\n",
    "    pratos = (\n",
    "        df_menu.loc[df_menu[\"categoria_corr\"] == cat, \"titulo\"]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    pratos = sorted(set([p for p in pratos if p and p.lower() != \"nan\"]))\n",
    "    return pratos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4236843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeea6e245e947f4987ab59f284830ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings locais gerados: (28, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = rag_dataset[\"chunks\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "E = model_st.encode(\n",
    "    texts,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "print(\"✅ Embeddings locais gerados:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6830d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VectorStore FAISS criado. Vetores: 28\n"
     ]
    }
   ],
   "source": [
    "dim = E.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)   # IP + normalizado => cosseno\n",
    "index.add(E)\n",
    "\n",
    "print(\"✅ VectorStore FAISS criado. Vetores:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae90f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_faiss(query: str, top_k: int = 10):\n",
    "    q = model_st.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, idx = index.search(q, top_k)\n",
    "\n",
    "    hits = rag_dataset.iloc[idx[0]].copy()\n",
    "    hits[\"score\"] = scores[0]\n",
    "    return hits.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d03b3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(rows, max_chars=4500):\n",
    "    parts, total = [], 0\n",
    "    for r in rows.itertuples():\n",
    "        tag = f\"[Fonte: {r.document_id} | chunk {r.chunk_id} | {r.path_arquivo}]\"\n",
    "        block = f\"{tag}\\n{str(r.chunks).strip()}\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "CATEGORIAS_OFICIAIS = [\"Tradicional\", \"Especialidade\", \"Salada\", \"Sobremesa\"]\n",
    "\n",
    "\n",
    "def hits_empty(query: str = \"\", fonte: str = \"rag_dataset_chunks.csv (consulta estruturada)\"):\n",
    "    return pd.DataFrame([{\n",
    "        \"document_id\": fonte,\n",
    "        \"chunk_id\": \"-\",\n",
    "        \"score\": 1.0,\n",
    "        \"categoria\": \"-\",\n",
    "        \"categoria_corr\": \"-\",\n",
    "        \"titulo\": \"-\",\n",
    "        \"tipo\": \"dataset\",\n",
    "        \"path_arquivo\": fonte\n",
    "    }])\n",
    "\n",
    "\n",
    "def listar_categorias():\n",
    "    return CATEGORIAS_OFICIAIS\n",
    "\n",
    "\n",
    "def extrair_categoria_da_pergunta(pergunta: str):\n",
    "    p = pergunta.lower()\n",
    "    if \"tradicion\" in p:\n",
    "        return \"Tradicional\"\n",
    "    if \"especial\" in p:\n",
    "        return \"Especialidade\"\n",
    "    if \"salad\" in p:\n",
    "        return \"Salada\"\n",
    "    if \"sobremes\" in p:\n",
    "        return \"Sobremesa\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def listar_pratos_da_categoria(cat: str):\n",
    "    df_menu = rag_dataset[rag_dataset[\"tipo\"].astype(str).str.lower() == \"pdf\"].copy()\n",
    "    pratos = (\n",
    "        df_menu.loc[df_menu[\"categoria_corr\"] == cat, \"titulo\"]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    return sorted(set([p for p in pratos if p and p.lower() != \"nan\"]))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# NOVO: detectar \"categoria de um prato\"\n",
    "# -----------------------------\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _build_menu_maps():\n",
    "    # usa apenas PDFs (pratos)\n",
    "    df_menu = rag_dataset[rag_dataset[\"tipo\"].astype(str).str.lower() == \"pdf\"][[\"titulo\", \"categoria_corr\"]].dropna()\n",
    "    df_menu = df_menu.drop_duplicates()\n",
    "\n",
    "    titulo_norm_to_orig = {_norm_text(t): t for t in df_menu[\"titulo\"].tolist()}\n",
    "    titulo_norm_to_cat = {_norm_text(t): c for t, c in zip(df_menu[\"titulo\"], df_menu[\"categoria_corr\"])}\n",
    "    return titulo_norm_to_orig, titulo_norm_to_cat\n",
    "\n",
    "\n",
    "# constrói mapas uma vez (reconstrói ao reexecutar a célula)\n",
    "TITULO_NORM_TO_ORIG, TITULO_NORM_TO_CAT = _build_menu_maps()\n",
    "\n",
    "\n",
    "def eh_pergunta_categoria_de_prato(pergunta: str) -> bool:\n",
    "    p = pergunta.lower()\n",
    "    gatilhos = [\n",
    "        \"qual a categoria\",\n",
    "        \"qual é a categoria\",\n",
    "        \"qual categoria\",\n",
    "        \"em qual categoria\",\n",
    "        \"categoria do prato\",\n",
    "        \"categoria da receita\",\n",
    "        \"esse prato é de qual categoria\",\n",
    "        \"essa receita é de qual categoria\",\n",
    "    ]\n",
    "    return (\"categoria\" in p) and any(g in p for g in gatilhos)\n",
    "\n",
    "\n",
    "def encontrar_prato_na_pergunta(pergunta: str):\n",
    "    qn = _norm_text(pergunta)\n",
    "\n",
    "    # 1) tenta achar título completo como substring\n",
    "    for tnorm in sorted(TITULO_NORM_TO_ORIG.keys(), key=len, reverse=True):\n",
    "        if tnorm and tnorm in qn:\n",
    "            return tnorm\n",
    "\n",
    "    # 2) fallback por overlap de palavras (mínimo 2)\n",
    "    q_tokens = set(qn.split())\n",
    "    best, best_score = None, 0\n",
    "    for tnorm in TITULO_NORM_TO_ORIG.keys():\n",
    "        t_tokens = set(tnorm.split())\n",
    "        score = len(q_tokens & t_tokens)\n",
    "        if score > best_score and score >= 2:\n",
    "            best, best_score = tnorm, score\n",
    "    return best\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Rotas / itents\n",
    "# -----------------------------\n",
    "def eh_pergunta_listar_itens_categoria(pergunta: str) -> bool:\n",
    "    p = pergunta.lower()\n",
    "    tem_intencao = any(\n",
    "        k in p\n",
    "        for k in [\n",
    "            \"liste\",\n",
    "            \"listar\",\n",
    "            \"quais pratos\",\n",
    "            \"quais itens\",\n",
    "            \"itens da categoria\",\n",
    "            \"pratos da categoria\",\n",
    "            \"menu da categoria\",\n",
    "        ]\n",
    "    )\n",
    "    return tem_intencao and (extrair_categoria_da_pergunta(pergunta) is not None)\n",
    "\n",
    "\n",
    "def eh_pergunta_de_categorias(pergunta: str) -> bool:\n",
    "    p = pergunta.lower().strip()\n",
    "\n",
    "    # não confundir com \"categoria de um prato\"\n",
    "    if eh_pergunta_categoria_de_prato(pergunta):\n",
    "        return False\n",
    "\n",
    "    gatilhos = [\n",
    "        \"quantas categorias\",\n",
    "        \"liste as categorias\",\n",
    "        \"listar categorias\",\n",
    "        \"quais sao as categorias\",\n",
    "        \"quais são as categorias\",\n",
    "        \"categorias do cardapio\",\n",
    "        \"categorias do cardápio\",\n",
    "        \"todas as categorias\",\n",
    "    ]\n",
    "    return any(g in p for g in gatilhos)\n",
    "\n",
    "\n",
    "def generate_answer(query: str, top_k: int = 5, temperature: float = 0.2, min_score: float = 0.28):\n",
    "    q = query.lower()\n",
    "\n",
    "    # 1) LISTAR PRATOS POR CATEGORIA (vem primeiro!)\n",
    "    if eh_pergunta_listar_itens_categoria(query):\n",
    "        cat = extrair_categoria_da_pergunta(query)\n",
    "        pratos = listar_pratos_da_categoria(cat)\n",
    "\n",
    "        if not pratos:\n",
    "            return f\"Não encontrei pratos para a categoria **{cat}** na base atual.\", hits_empty()\n",
    "\n",
    "        texto = f\"Pratos da categoria **{cat}**:\\n- \" + \"\\n- \".join(pratos)\n",
    "        texto += f\"\\n\\nTotal: {len(pratos)} pratos.\"\n",
    "        return texto, hits_empty(query, f\"rag_dataset_chunks.csv (lista de pratos: {cat})\")\n",
    "\n",
    "    # 2) CATEGORIA DE UM PRATO ESPECÍFICO\n",
    "    if eh_pergunta_categoria_de_prato(query):\n",
    "        tnorm = encontrar_prato_na_pergunta(query)\n",
    "        if tnorm and tnorm in TITULO_NORM_TO_CAT:\n",
    "            prato = TITULO_NORM_TO_ORIG[tnorm]\n",
    "            cat = TITULO_NORM_TO_CAT[tnorm]\n",
    "            return f\"O prato **{prato}** fica na categoria **{cat}**.\", hits_empty(query, f\"rag_dataset_chunks.csv (match título: {prato})\")\n",
    "        return (\n",
    "            \"Não consegui identificar o nome do prato na sua pergunta. \"\n",
    "            \"Digite o nome exato do prato (como no cardápio) que eu te falo a categoria.\",\n",
    "            hits_empty(),\n",
    "        )\n",
    "\n",
    "    # 3) LISTAR CATEGORIAS DO CARDÁPIO\n",
    "    if eh_pergunta_de_categorias(query):\n",
    "        cats = listar_categorias()\n",
    "        if \"quantas\" in q:\n",
    "            texto = f\"No cardápio existem {len(cats)} categorias:\\n- \" + \"\\n- \".join(cats)\n",
    "        else:\n",
    "            texto = \"As categorias no cardápio são:\\n- \" + \"\\n- \".join(cats)\n",
    "        texto += f\"\\n\\nTotal: {len(cats)} categorias.\"\n",
    "        return texto, hits_empty(query, \"rag_dataset_chunks.csv (categorias oficiais)\")\n",
    "\n",
    "    # 4) RAG NORMAL\n",
    "    hits = retrieve_faiss(query, top_k=max(top_k, 20))\n",
    "    if min_score is not None:\n",
    "        hits = hits[hits[\"score\"] >= min_score]\n",
    "\n",
    "    # OBS: se preço/detalhe vier ruim, troque por hits = hits.head(top_k).copy()\n",
    "    hits = hits.drop_duplicates(subset=[\"document_id\"]).head(top_k).copy()\n",
    "\n",
    "    context = format_context(hits)\n",
    "\n",
    "    system = (\n",
    "        \"Você é um assistente de um restaurante. \"\n",
    "        \"Responda SOMENTE com base no CONTEXTO fornecido. \"\n",
    "        \"NUNCA invente categorias, pratos ou preços. \"\n",
    "        \"Se o contexto não tiver a informação, diga que não encontrou na base. \"\n",
    "        \"No final, liste as fontes usadas no formato: document_id (chunk_id).\"\n",
    "    )\n",
    "\n",
    "    user = f\"PERGUNTA:\\n{query}\\n\\nCONTEXTO:\\n{context}\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOY,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content, hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66973c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Mapa de pratos -> categoria (apenas PDFs = pratos do cardápio)\n",
    "_TITULOS_MENU = (\n",
    "    rag_dataset[rag_dataset[\"tipo\"].astype(str).str.lower() == \"pdf\"][[\"titulo\",\"categoria_corr\"]]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "TITULO_NORM_TO_ORIG = { _norm_text(t): t for t in _TITULOS_MENU[\"titulo\"].tolist() }\n",
    "TITULO_NORM_TO_CAT  = { _norm_text(t): c for t,c in zip(_TITULOS_MENU[\"titulo\"], _TITULOS_MENU[\"categoria_corr\"]) }\n",
    "\n",
    "def eh_pergunta_categoria_de_prato(pergunta: str) -> bool:\n",
    "    p = pergunta.lower()\n",
    "    # exemplos: \"qual a categoria da rabada\", \"em qual categoria fica rabada\"\n",
    "    gatilhos = [\n",
    "        \"qual a categoria\", \"qual é a categoria\", \"qual categoria\",\n",
    "        \"em qual categoria\", \"essa receita é de qual categoria\",\n",
    "        \"esse prato é de qual categoria\", \"categoria do prato\", \"categoria da receita\"\n",
    "    ]\n",
    "    return (\"categoria\" in p) and any(g in p for g in gatilhos)\n",
    "\n",
    "def encontrar_prato_na_pergunta(pergunta: str) -> str | None:\n",
    "    qn = _norm_text(pergunta)\n",
    "\n",
    "    # tenta match por substring do título no texto da pergunta (mais confiável)\n",
    "    # ordena por tamanho desc pra pegar títulos maiores primeiro\n",
    "    for tnorm in sorted(TITULO_NORM_TO_ORIG.keys(), key=len, reverse=True):\n",
    "        if tnorm and tnorm in qn:\n",
    "            return tnorm\n",
    "\n",
    "    # fallback: tenta match por palavras (interseção)\n",
    "    q_tokens = set(qn.split())\n",
    "    best, best_score = None, 0\n",
    "    for tnorm in TITULO_NORM_TO_ORIG.keys():\n",
    "        t_tokens = set(tnorm.split())\n",
    "        score = len(q_tokens & t_tokens)\n",
    "        if score > best_score and score >= 2:  # pelo menos 2 palavras em comum\n",
    "            best, best_score = tnorm, score\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a93fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cardápio existem 4 categorias:\n",
      "- Tradicional\n",
      "- Especialidade\n",
      "- Salada\n",
      "- Sobremesa\n",
      "\n",
      "Total: 4 categorias.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>score</th>\n",
       "      <th>titulo</th>\n",
       "      <th>path_arquivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rag_dataset_chunks.csv (categorias oficiais)</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>rag_dataset_chunks.csv (categorias oficiais)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    document_id chunk_id  score titulo  \\\n",
       "0  rag_dataset_chunks.csv (categorias oficiais)        -    1.0      -   \n",
       "\n",
       "                                   path_arquivo  \n",
       "0  rag_dataset_chunks.csv (categorias oficiais)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans, used = generate_answer(\"Quantas categorias possui\", top_k=5)\n",
    "print(ans)\n",
    "\n",
    "if used is not None and len(used) > 0:\n",
    "    used_cols = [c for c in [\"document_id\",\"chunk_id\",\"score\",\"categoria_norm\",\"titulo\",\"path_arquivo\"] if c in used.columns]\n",
    "    display(used[used_cols].head(1))\n",
    "else:\n",
    "    print(\"Sem fontes para exibir (resposta não usou RAG ou não houve hits).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
